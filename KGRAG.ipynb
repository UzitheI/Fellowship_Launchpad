{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: google-genai in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-genai) (2.38.0)\n",
      "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-genai) (2.10.6)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-genai) (2.32.2)\n",
      "Requirement already satisfied: websockets<15.0dev,>=13.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-genai) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2024.12.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.3.17)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-chroma in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: chromadb in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (0.3.35)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (2.32.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.115.8)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (3.14.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0rc1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Requirement already satisfied: uuid in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (1.30)\n",
      "Requirement already satisfied: chromadb in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.115.8)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (3.14.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from chromadb) (13.3.5)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.2)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0rc1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.51b0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (2.0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\cwj19\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sentence-transformer (from versions: none)\n",
      "ERROR: No matching distribution found for sentence-transformer\n"
     ]
    }
   ],
   "source": [
    "#Install all relevant packages for the following sets of code. Note that you should also have pulled the llama3:8b model\n",
    "#for the current retrieval systems\n",
    "!pip install PyPDF2\n",
    "!pip install google-genai\n",
    "!pip install pandas\n",
    "!pip install langchain langchain-community langchain-ollama langchain-chroma chromadb\n",
    "!pip install uuid\n",
    "!pip install chromadb\n",
    "!pip install sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load necessary functions and packages for all of the following code\n",
    "from PyPDF2 import PdfReader\n",
    "from google import genai\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from langchain_ollama import OllamaEmbeddings,OllamaLLM\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "import json\n",
    "\n",
    "#Initialize the Gemini client for the calls to an LLM during the rule extraction and code generation portions of the pipeline\n",
    "client = genai.Client(api_key=\"AIzaSyCn7ei90S-T4YxpLhqsxrEgg0bJToEFjM8\")\n",
    "\n",
    "#Initialize the embedding model and llm for the embedding, entity extraction, and retrieval portions of the pipeline\n",
    "vec_embed_model = OllamaEmbeddings(model=\"llama3:8b\")\n",
    "table_embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "llm=OllamaLLM(model=\"llama3:8b\",temperature=0.7)\n",
    "\n",
    "#Provide the names of the rule document and table schema for the loading of data.\n",
    "pdf_file=\"Synthetic_Financial_Rules.pdf\"\n",
    "tab_file=\"ReadableDataDict.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Extraction and Knowledge Graph Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The buildJSONStructural() function takes as argument the name of a PDF document containing financial rules and provides as \n",
    "#output JSON that represents the nodes and edges of a knowledge graph where the base node represents the document as a whole,\n",
    "#the secondary nodes represent the sections of the document, and the tertiary nodes represent each extracted rule.\n",
    "def buildJSONStructural(file_name):\n",
    "    #Load the rule document into a single string\n",
    "    with open(file_name,'rb') as pdf_file:\n",
    "        pdf_reader=PdfReader(pdf_file)\n",
    "        text=\"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page=pdf_reader.pages[page_num]\n",
    "            text+=page.extract_text()\n",
    "\n",
    "    #The nodes and relationships are extracted from the document by calling to the Gemini LLM. The prompt is engineered as follows:\n",
    "    \n",
    "    #The text of the document is loaded into the prompt. The LLM is told that the output should be in a JSON form with both nodes and relationships provided. \n",
    "    rule_extract_prompt='Decompose the text of the document:\\n\\n'+text+'\\n\\n into a .JSON format representing the nodes '\n",
    "    rule_extract_prompt+='and relationships of a knowledge graph. The format of the output should take the form:\\n'\n",
    "\n",
    "    #An example of what the JSON output should look like is provided to the LLM. This helps the LLM to always structure the\n",
    "    #output with particular element names that make processing the output string into JSON possible.\n",
    "    rule_extract_prompt+=' \"\"\"json\\n {\"nodes\": [{\"id\":\"doc_1\",\"type\":\"Document\",\"metadata\":{\"id\":\"doc_1\",\"title\":\"ExampleDocTitle\",\"purpose\":\"ExampleDocPurpose\"}},'\n",
    "    rule_extract_prompt+='{\"id\":\"sec_1\",\"type\":\"Section\",\"metadata\":{\"id\":\"sec_1\",\"title\":\"ExampleSecTitle\",\"number\":\"ExampleSecNum\",\"scope_tags\":[\"ExampleScopeTag\"]}},'\n",
    "    rule_extract_prompt+='{\"id\":\"rule_1_1\",\"type\":\"Rule\",\"metadata\":{\"id\":\"rule_1_1\",\"number\":\"ExampleRuleNum\",\"text\":\"ExampleRuleText\"}}],'\n",
    "    rule_extract_prompt+='\"relationships\":[{\"source_id\":\"doc_1\",\"target_id\":\"sec_1\",\"type\":\"Contains\"},{\"source_id\":\"sec_1\",\"target_id\":\"rule_1_1\",\"type\":\"Establishes\"}]}\"\"\"\\n'\n",
    "\n",
    "    #The exact contents of each node type along with an explanation of their metadata elements is provided to minimize the \n",
    "    #chance of the LLM providing undesired metadata contents\n",
    "    rule_extract_prompt+='The three node types are in detail:\\n'\n",
    "\n",
    "    #The base (Document) node contains data on the document as a whole including its 'title' and 'purpose'\n",
    "    rule_extract_prompt+='A Document node that represents the document as a whole. This node contains three metadata elements: \"id\" which is the ID of the node;'\n",
    "    rule_extract_prompt+=' \"title\" which is the title of the document; \"purpose\" which is the description of the overall purpose of the document.\\n'\n",
    "\n",
    "    #The secondary (Section) nodes contain data on: the given 'title' of the section, where the sections appear in the document (represented as 'number'),\n",
    "    #and the broad scope of the rules (represented as 'scope_tags') contained in the rules of each section and give a specific list of the values the tags  \n",
    "    rule_extract_prompt+='Section nodes that represent the distinct sections of the document. These nodes contain four metadata elements: \"id\" which is the ID '\n",
    "    rule_extract_prompt+=' of the node; \"title\" which is the verbatim title of the given section; \"number\" which is the number associated with the positional '\n",
    "    rule_extract_prompt+='occurrence of the section in the document and follows the format \"Section #\"; \"scope_tags\" which is a list of elements indicating the '\n",
    "    rule_extract_prompt+='particular scope the rules in the section cover and take values of \"consumer_benefits\", \"deadlines\", \"fees\", \"service_limits\", and/or \"consumer_liability\".\\n'\n",
    "\n",
    "    #The tertiary (Rule) nodes contain data on: where the rules appear in each section (represented as 'number') and the verbatim 'text' of each rule\n",
    "    rule_extract_prompt+='Rule nodes that represent the distinct rules of each section. These nodes contain three metadata elements: \"id\" which is the ID of the node;'\n",
    "    rule_extract_prompt+=' \"number\" which is the number associated with the positional occurrence of the rule in each section and follows the format \"Section #.(#)\"; ' \n",
    "    rule_extract_prompt+=' \"text\" which is the verbatim text of the rule.\\n'\n",
    "\n",
    "    #The exact contents of each relationship type is provided to minimize the LLM identifying non-existant relationships\n",
    "    rule_extract_prompt+='The three relationship types, depending on the structure of the document, are in detail:\\n'\n",
    "\n",
    "    #Specify that the Document and Section nodes are connected by Contains relationships\n",
    "    rule_extract_prompt+='Document nodes identified by their \"id\" metadata and Section nodes identified by their \"id\" metadata can be related by '\n",
    "    rule_extract_prompt+=' \"Contains\" relationships.'\n",
    "\n",
    "    #Specify that the Section and Rule nodes are connected by Establishes relationships\n",
    "    rule_extract_prompt+='Section nodes identified by their \"id\" metadata and Rule nodes identified by their \"id\" metadata can be related by '\n",
    "    rule_extract_prompt+=' \"Establishes\" relationships.'\n",
    "\n",
    "    #Tell the LLM to only output the requested JSON document. This makes the processing of the string output into JSON possible consistently\n",
    "    rule_extract_prompt=rule_extract_prompt+'Output only the set of .JSON data.'\n",
    "\n",
    "    #Feed the prompt as built above to Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\", contents=rule_extract_prompt)\n",
    "    #Store the LLM response as a string\n",
    "    extracted_rules=response.text\n",
    "    #Convert the LLM string to an actual JSON dictionary and return that dictionary\n",
    "    json_rules=json.loads(extracted_rules[7:len(extracted_rules)-3])\n",
    "    return json_rules\n",
    "\n",
    "#The buildJSONSemantic() function takes as argument the name of a PDF document containing financial rules and provides as \n",
    "#output JSON that represents the nodes and edges of a knowledge graph where the base node represents the broad scope of the rules in the document,\n",
    "#the secondary nodes represent the type of incentive a particular rule is, and the tertiary nodes represent each extracted rule.\n",
    "def buildJSONSemantic(file_name):\n",
    "    #Load the rule document into a single string\n",
    "    with open(file_name,'rb') as pdf_file:\n",
    "        pdf_reader=PdfReader(pdf_file)\n",
    "        text=\"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page=pdf_reader.pages[page_num]\n",
    "            text+=page.extract_text()\n",
    "\n",
    "    #The nodes and relationships are extracted from the document by calling to the Gemini LLM. The prompt is engineered as follows:\n",
    "\n",
    "    #The text of the document is loaded into the prompt. The LLM is told that the output should be in a JSON form with both nodes and relationships provided.  \n",
    "    rule_extract_prompt='Decompose the text of the document:\\n\\n\"+text+\"\\n\\n into a .JSON format representing the nodes '\n",
    "    rule_extract_prompt+='and relationships of a knowledge graph. The format of the output should take the form:\\n'\n",
    "\n",
    "    #An example of what the JSON output should look like is provided to the LLM. This helps the LLM to always structure the\n",
    "    #output with particular element names that make processing the output string into JSON possible.\n",
    "    rule_extract_prompt+='\"\"\"json\\n {\"nodes\": [{\"id\":\"scope_1\",\"type\":\"Scope\",\"metadata\":{\"id\":\"scope_1\",\"title\":\"ScopeTypeExample\"}},'\n",
    "    rule_extract_prompt+='{\"id\":\"inc_1\",\"type\":\"Incentive\",\"metadata\":{\"id\":\"inc_1\",\"title\":\"IncentiveTypeExample\",\"description\":\"ExampleDescription\"}},'\n",
    "    rule_extract_prompt+='{\"id\":\"rule_1\",\"type\":\"Rule\",\"metadata\":{\"id\":\"rule_1\",\"number\":\"ExampleRuleNum\",\"text\":\"ExampleRuleText\"}}],'\n",
    "    rule_extract_prompt+='\"relationships\":[{\"source_id\":\"inc_1\",\"target_id\":\"scope_1\",\"type\":\"Serves_As\"},{\"source_id\":\"inc_1\",\"target_id\":\"inc_1\",\"type\":\"Supports\"}]}\"\"\"\\n'\n",
    "    \n",
    "    #The exact contents of each node type along with an explanation of their metadata elements is provided to minimize the \n",
    "    #chance of the LLM providing undesired metadata contents\n",
    "    rule_extract_prompt+='The three node types are in detail:\\n'\n",
    "\n",
    "    #The base (Scope) node contains a short 'title' that is meant to represent the overall type of financial action the rules dictate. \n",
    "    #Note the 'title' is generated by the LLM itself and is therefore dependent on semantic understanding. \n",
    "    rule_extract_prompt+='A Scope node that represents the overall type of financial action that the rules of the document regulate. This node contains '\n",
    "    rule_extract_prompt+=' two metadata elements: \"id\" which is the unique ID of the node; \"title\" which is a one or two word description of the relevant '\n",
    "    rule_extract_prompt+=' financial action such as \"Consumer Transactions\" or \"Investments\".'\n",
    "\n",
    "\n",
    "    #The secondary (Incentive) nodes contain data on: whether a rule is a positive, neutral, or negative incentive in the 'title' element and a 'description'\n",
    "    #of what consitutes a positive, neutral, or negative incentive with respect to the type of financial action associated with the (Scope) node.\n",
    "    #Note the 'description' of each node is generated by the LLM itself and is therefore dependent on semantic understanding. \n",
    "    rule_extract_prompt+='Three Incentive nodes that represent the distinct types of incentive that each rule serves as for the given financial action. '\n",
    "    rule_extract_prompt+='These nodes contain three metadata elements: \"id\" which is the unique ID of the node; \"title\" which is the name of the type of '\n",
    "    rule_extract_prompt+='incentive that a rule serves as and takes one of three values \"Positive Incentive\", \"Negative Incentive\", or \"Neutral Incentive\"; '\n",
    "    rule_extract_prompt+='\"description\" which is a brief description of what is meant by an incentive being positive, negative or neutral with respect to the '\n",
    "    rule_extract_prompt+='type of financial action in the Scope node.'\n",
    "\n",
    "    #The tertiary (Rule) nodes contain data on: the occurence of each rule (represented as 'number') and the verbatim 'text' of each rule\n",
    "    rule_extract_prompt+='Rule nodes that represent the distinct rules of the document. These nodes contain three metadata elements: \"id\" which is the ID of the node;'\n",
    "    rule_extract_prompt+=' \"number\" which is the number associated with the positional occurrence of the rule in the document and follows the format \"Rule #\"; ' \n",
    "    rule_extract_prompt+=' \"text\" which is the verbatim text of the rule.\\n'\n",
    "\n",
    "    #Specify that the Scope and Incentive nodes are connected by Serves_As relationships\n",
    "    rule_extract_prompt+='Scope nodes identified by their \"id\" metadata and Incentive nodes identified by their \"id\" metadata can be related by '\n",
    "    rule_extract_prompt+=' \"Serves_As\" relationships.'\n",
    "\n",
    "    #Specify that the Incentive and Rule nodes are connected by Supports relationships\n",
    "    rule_extract_prompt+='Incentive nodes identified by their \"id\" metadata and Rule nodes identified by their \"id\" metadata can be related by '\n",
    "    rule_extract_prompt+=' \"Supports\" relationships.'\n",
    "\n",
    "    #Tell the LLM to only output the requested JSON document. This makes the processing of the string output into JSON possible consistently\n",
    "    rule_extract_prompt=rule_extract_prompt+'Output only the set of .JSON data.'\n",
    "\n",
    "    #Feed the prompt as build to Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\", contents=rule_extract_prompt)\n",
    "    #Store the LLM response as a string\n",
    "    extracted_rules=response.text\n",
    "    #Convert the LLM string to an actual JSON dictionary and return that dictionary\n",
    "    json_rules=json.loads(extracted_rules[7:len(extracted_rules)-3])\n",
    "    return json_rules\n",
    "\n",
    "#The neo4jFromJSONStructural() function takes as argument a JSON dictionary in the specific format specified in buildJSONStructural() and processes \n",
    "#the contents of the JSON dictionary into Cypher \"Merge\" queries that are stored in a string. This string is the output of the\n",
    "#function and can be sent through the Neo4j query() function to build/update a Neo4j database.\n",
    "def neo4jFromJSONStructural(json_dict):\n",
    "    #Pull the list of nodes of the knowledge graph from the dictionary and store them in a temporary list.\n",
    "    node_lst=json_dict['nodes']\n",
    "\n",
    "    #Pull the list of relationships of the knowledge graph from the dictionary and store them in a temporary list.\n",
    "    rel_lst=json_dict['relationships']\n",
    "\n",
    "    #Create an empty string for the Cypher query that generates/updates the nodes of the database\n",
    "    cypher_query1=\"\"\n",
    "\n",
    "    #Create an empty string for the Cypher query that generate/updates the relationships of the database\n",
    "    cypher_query2=\"\"\n",
    "\n",
    "    #Create two empty strings where the first will \"Match\" the relevant nodes and the second will \"Merge\" the edges that connected the relevant nodes\n",
    "    cypher_query2a=\"\"\n",
    "    cypher_query2b=\"\"\n",
    "\n",
    "    #Iterate through the list of nodes\n",
    "    for j in node_lst:\n",
    "        #Create an empty string to store the metadata information for a given node that processes correctly from the Cypher query\n",
    "        mtext=\"\"\n",
    "        ind=0\n",
    "\n",
    "        #Iterate over the key, value pairs of the metadata dictionary to format the string properly\n",
    "        for k,v in j['metadata'].items():\n",
    "            #Use an if-else statement to make sure that the last element of the metadata dictionary is not followed by a comma in the Cypher query. \n",
    "            #This assures that the Cypher call works properly\n",
    "            if ind<len(j['metadata'])-1:\n",
    "                #Use an if-else statement to make sure list and string type elements of the metadata dictionary are added to the Cypher query properly.\n",
    "                if str(v)[0]=='[' and str(v)[-1]==']':\n",
    "                    mtext+=k+\":\"+str(v)+\", \"\n",
    "                else:\n",
    "                    mtext+=k+\":'\"+str(v)+\"', \"\n",
    "            else:\n",
    "                #Use an if-else statement to make sure list and string type elements of the metadata dictionary are added to the Cypher query properly.\n",
    "                if str(v)[0]=='[' and str(v)[-1]==']':\n",
    "                    mtext+=k+\":\"+str(v)\n",
    "                else:\n",
    "                    mtext+=k+\":'\"+str(v)+\"'\"\n",
    "            ind+=1\n",
    "\n",
    "        #Once the metadata dictionary has been properly formatted using the above loop, load the node ID, node type, and node metadata into a \"Merge\" call\n",
    "        #and add this to the node generation string.\n",
    "        text=\"MERGE (\"+j['id']+\":\"+j['type']+\" {\"+mtext+\"})\\n\"\n",
    "        cypher_query1+=text\n",
    "\n",
    "    #Iterate through the list of relationships    \n",
    "    for j in rel_lst:\n",
    "        #Use an if-else statement to distinguish between the two types of relationships\n",
    "        if j['type']=='Contains':\n",
    "            #Load all \"Match\" statements into one substring\n",
    "            cypher_query2a+=\"MATCH (\"+j['source_id']+\":Document {id: '\"+j['source_id']+\"'}), (\"+j['target_id']+\":Section {id: '\"+j['target_id']+\"'})\\n\"\n",
    "            #Load all \"Merge\" statements into one substring\n",
    "            cypher_query2b+=\"MERGE (\"+j['source_id']+\")-[:Contains]->(\"+ j['target_id']+\")\\n\"\n",
    "        elif j['type']=='Establishes':\n",
    "            #Load all \"Match\" statements into one substring\n",
    "            cypher_query2a+=\"MATCH (\"+j['source_id']+\":Section {id: '\"+j['source_id']+\"'}), (\"+j['target_id']+\":Rule {id: '\"+j['target_id']+\"'})\\n\"\n",
    "            #Load all \"Match\" statements into one substring\n",
    "            cypher_query2b+=\"MERGE (\"+j['source_id']+\")-[:Establishes]->(\"+ j['target_id']+\")\\n\"\n",
    "    \n",
    "    #Bring together both substrings so all \"Match\" statements precede the \"Merge\" statements. This assures the Cypher query does not throw an error.\n",
    "    cypher_query2=cypher_query2a+cypher_query2b\n",
    "\n",
    "    #Return the node creation/update and relationship creation/update queries as separate strings so that the node creation/update precedes the\n",
    "    #relationship creation/update\n",
    "    return [cypher_query1,cypher_query2]\n",
    "\n",
    "#The neo4jFromJSONSemantic() function takes as argument a JSON dictionary in the specific format specified in buildJSONSemantic() and processes \n",
    "#the contents of the JSON dictionary into Cypher \"Merge\" queries that are stored in a string. This string is the output of the\n",
    "#function and can be sent through the Neo4j query() function to build/update a Neo4j database.\n",
    "def neo4jFromJSONSemantic(json_dict):\n",
    "    #Pull the list of nodes of the knowledge graph from the dictionary and store them in a temporary list.\n",
    "    node_lst=json_dict['nodes']\n",
    "\n",
    "    #Pull the list of relationships of the knowledge graph from the dictionary and store them in a temporary list.\n",
    "    rel_lst=json_dict['relationships']\n",
    "\n",
    "    #Create an empty string for the Cypher query that generates/updates the nodes of the database\n",
    "    cypher_query1=\"\"\n",
    "\n",
    "    #Create an empty string for the Cypher query that generate/updates the relationships of the database\n",
    "    cypher_query2=\"\"\n",
    "\n",
    "    #Create two empty strings where the first will \"Match\" the relevant nodes and the second will \"Merge\" the edges that connected the relevant nodes\n",
    "    cypher_query2a=\"\"\n",
    "    cypher_query2b=\"\"\n",
    "\n",
    "    #Iterate through the list of nodes\n",
    "    for j in node_lst:\n",
    "        #Create an empty string to store the metadata information for a given node that processes correctly from the Cypher query\n",
    "        mtext=\"\"\n",
    "        ind=0\n",
    "\n",
    "        #Iterate over the key, value pairs of the metadata dictionary to format the string properly\n",
    "        for k,v in j['metadata'].items():\n",
    "            #Use an if-else statement to make sure that the last element of the metadata dictionary is not followed by a comma in the Cypher query. \n",
    "            #This assures that the Cypher call works properly\n",
    "            if ind<len(j['metadata'])-1:\n",
    "                mtext+=k+\":'\"+str(v)+\"', \"\n",
    "            else:\n",
    "                mtext+=k+\":'\"+str(v)+\"'\"\n",
    "            ind+=1\n",
    "\n",
    "        #Once the metadata dictionary has been properly formatted using the above loop, load the node ID, node type, and node metadata into a \"Merge\" call\n",
    "        #and add this to the node generation string.\n",
    "        text=\"MERGE (\"+j['id']+\":\"+j['type']+\" {\"+mtext+\"})\\n\"\n",
    "        cypher_query1+=text\n",
    "\n",
    "    #Iterate through the list of relationships\n",
    "    for j in rel_lst:\n",
    "        #Use an if-else statement to distinguish between the two types of relationships\n",
    "        if j['type']=='Serves_As':\n",
    "            #Load all \"Match\" statements into one substring\n",
    "            cypher_query2a+=\"MATCH (\"+j['source_id']+\":Incentive {id: '\"+j['source_id']+\"'}), (\"+j['target_id']+\":Scope {id: '\"+j['target_id']+\"'})\\n\"\n",
    "            #Load all \"Merge\" statements into one substring\n",
    "            cypher_query2b+=\" MERGE (\"+j['source_id']+\")-[:Serves_As]->(\"+ j['target_id']+\")\\n\"\n",
    "        elif j['type']=='Supports':\n",
    "            #Load all \"Match\" statements into one substring\n",
    "            cypher_query2a+=\"MATCH (\"+j['source_id']+\":Incentive {id: '\"+j['source_id']+\"'}), (\"+j['target_id']+\":Rule {id: '\"+j['target_id']+\"'})\\n\"\n",
    "            #Load all \"Merge\" statements into one substring\n",
    "            cypher_query2b+=\" MERGE (\"+j['source_id']+\")<-[:Supports]-(\"+ j['target_id']+\")\\n\"\n",
    "    \n",
    "    #Bring together both substrings so all \"Match\" statements precede the \"Merge\" statements. This assures the Cypher query does not throw an error.\n",
    "    cypher_query2=cypher_query2a+cypher_query2b\n",
    "\n",
    "    #Return the node creation/update and relationship creation/update queries as separate strings so that the node creation/update precedes the\n",
    "    #relationship creation/update\n",
    "    return [cypher_query1,cypher_query2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule and Column Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tableRetrieverInit() function takes as argument the file name of a data dictionary and access or creates a Chroma vector database that is associated with \n",
    "#the contents of this document. The Collection object that allows for queries of this database is the output of the function and can be queried in order\n",
    "#to retrieve the column and table names associated with the data dictionary.\n",
    "def tableRetrieverInit(table_file):\n",
    "    #Load the data dictionary as a Pandas dataframe\n",
    "    df=pd.read_excel(table_file)\n",
    "\n",
    "    #Give a name to the Chroma database\n",
    "    coll_name=\"my_collection\"\n",
    "    #Initialize a Chroma client with a persistent, local directory\n",
    "    chr_client=chromadb.PersistentClient(path='./TableSchemaVec')\n",
    "    \n",
    "    #Connect to or create a Chroma database where any similarity calculations are done using cosine similarity\n",
    "    collection=chr_client.get_or_create_collection(name=coll_name,metadata={\"hnsw:space\":\"cosine\"})\n",
    "    refill_database=False #If the collection is empty or needs to be updated, set this boolean to True. Otherwise, this boolean is false\n",
    "\n",
    "    if refill_database:\n",
    "        #Initialize lists for all of the contents of the Chroma database\n",
    "        doc_ls=[]\n",
    "        embed_ls=[]\n",
    "        meta_ls=[]\n",
    "        id_ls=[]\n",
    "        count=0\n",
    "\n",
    "        #Iterate through all of the contents of the pandas dataframe\n",
    "        for index,row in df.iterrows():\n",
    "            #Fill a temporary variable with the text that is going to be vector embedded. Note that three options are given here to test how recall/precision\n",
    "            #can be tuned without changing the embedding model.\n",
    "            text=\"Table: \"+row['Table Name']+\"; Column: \"+row['Column Name']+\"; Description: \"+row['Description']\n",
    "            #text=row['Description']\n",
    "            #text=row['Column Name']\n",
    "            emb=table_embed_model.encode(text) #Calculate the embeddings for the text of an entry of the vector database\n",
    "            doc_ls.append(text) #Add the text to the list of documents\n",
    "            embed_ls.append(emb) #Add the embedding to the list of embeddings\n",
    "            meta_ls.append({\"table\":row['Table Name'],\"column\":row[\"Column Name\"]}) #Add the table and column names to the list of metadata entries\n",
    "            id_ls.append(\"id\"+str(count)) #Add a new id to the list of ids\n",
    "            count+=1 #Iterate the numeric used for the list of ids\n",
    "        collection.upsert(documents=doc_ls,embeddings=embed_ls,metadatas=meta_ls,ids=id_ls) #Add/update the entries of the vector database\n",
    "    #Return the Collection object\n",
    "    return collection\n",
    "\n",
    "#The graphRetrieverInit() function takes as argument the url, username, and password of a Neo4j graph database and accesses that database to add vector embeddings\n",
    "#to the Rule nodes based on the text of the nodes. The vector embedded version of the Neo4j database is the output of the function and similarity searches\n",
    "#can be performed on it in order to retrieve the text of rules.\n",
    "def graphRetrieverInit(uri,user,passw):\n",
    "    #Generate vector embeddings on rule nodes of knowledge graph\n",
    "    neo4j_vector=Neo4jVector.from_existing_graph(embedding=vec_embed_model, url=uri, username=user, password=passw,\n",
    "                                                 index_name='node_vector_ind',node_label=\"Rule\",text_node_properties=['text'],\n",
    "                                                 embedding_node_property='embedding')\n",
    "    #Return the vector embedded version of the database\n",
    "    return neo4j_vector\n",
    "\n",
    "#The invokeRetriever() function takes as arguments the name of a vector embedded database, the type of database (\"table\" or \"graph\") of ret, the text that is checked\n",
    "#against for the retrieval, and the number of elements that are to be retrieved. Because the code generator will need table information and rule texts in order\n",
    "#to work, the function will return a list of table names in the case that the \"table\" database is retrieved from and will return a list of rule texts in the \n",
    "#case that the \"graph\" database is retrieved from.\n",
    "def invokeRetriver(ret,ret_type,q_text,n_res):\n",
    "    #Use an if-else statement to differentiate between the databases types\n",
    "    if ret_type==\"table\":\n",
    "        #Use the text of a rule as the 'query' and retrieve the n_res column and table names most similar to the rule text\n",
    "        res=ret.query(query_texts=[q_text],n_results=n_res,include=[\"metadatas\"])\n",
    "        table_ls=[]\n",
    "        #Iterate through the n_res results\n",
    "        for j in res[\"metadatas\"][0]:\n",
    "            if j['table'] not in table_ls:\n",
    "                table_ls.append(j['table']) #Load the unique table names of the retrieved columns into a list\n",
    "        #Return the list of table names\n",
    "        return table_ls\n",
    "    elif ret_type==\"graph\":\n",
    "        #Use the text associated with a given set of tables the 'query' and retrieve the n_res rules most similar to the table text based on cosine similarity\n",
    "        res=ret.similarity_search(q_text,k=n_res)\n",
    "        out_ls=[]\n",
    "        #Iterate through the n_res results\n",
    "        for ru in res:\n",
    "                out_ls.append(ru.page_content[6:]) #Load the text of the retrieved rules into a list\n",
    "        #Return the list of rules\n",
    "        return out_ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The codeGenerator() function takes as arguments the name of the data dictionary and a dictionary that contains as entries the text of a rule and a list of \n",
    "#table names. This information is used in a prompt that is fed to Gemini in order to output JSON that indicates whether a SQL query or a message was generated,\n",
    "#the query itself or the text of the message, and an explanation of how the code checks the non-compliance of the rule or what extra information needs to be \n",
    "#included in the data dictionary in order to generate a SQL query. \n",
    "def codeGenerator(data,rets):\n",
    "    #Load the text of the rule into the prompt\n",
    "    code_gen_prompt=\"Given the rule:\\n\"+rets['rule']+\"\\nand the tables: \\n\"\n",
    "    tab_name_text=\"\" #Initialize a string that will contain the names of all tables from the data dictionary that the query can use\n",
    "    for t in range(len(rets['table_names'])):\n",
    "        #Use an if-else statement to differentiate how the last table name and all others are added to the string\n",
    "        if t<len(rets['table_names'])-1:\n",
    "            tab_name_text+=rets['table_names'][t].lower().replace(\" \",\"\")+\", \" #Add all but the last table name to the string. Assure names are lowercase with no spaces\n",
    "        else:\n",
    "            tab_name_text+=\"and \"+rets['table_names'][t].lower().replace(\" \",\"\") #Add the last table name to the string. Assure names are lowercase with no spaces\n",
    "        \n",
    "        #Add the information about the tables to the list. Make sure that the table names and column names are all lowercase and have no spaces in order to\n",
    "        #cause as few problems for the SQL query as possible\n",
    "        col_name_temp=\"\\nTable: \"+rets['table_names'][t].lower().replace(\" \",\"\")+\" which is composed of the following columns:\\n\"\n",
    "        for j in range(len(data['Table Name'])):\n",
    "            #Use an if-else statement to make sure that only the information associated with the retrieval is provided to Gemini and to differentiate between\n",
    "            #how the last and all other columns of a relevant table are added to the prompt\n",
    "            if (j<len(data['Table Name'])-1 and data['Table Name'][j].lower().replace(\" \",\"\")==rets['table_names'][t].lower().replace(\" \",\"\") and data['Table Name'][j+1].lower().replace(\" \",\"\")==rets['table_names'][t].lower().replace(\" \",\"\")):\n",
    "                col_name_temp+=\"Column: \"+data['Column Name'][j].lower().replace(\" \",\"\")+\"; Description: \"+data['Description'][j]+\"\\n\"\n",
    "            elif (j<len(data['Table Name'])-1 and data['Table Name'][j].lower().replace(\" \",\"\")==rets['table_names'][t].lower().replace(\" \",\"\") and data['Table Name'][j+1].lower().replace(\" \",\"\")!=rets['table_names'][t].lower().replace(\" \",\"\")) or (j==len(data['Table Name'])-1 and data['Table Name'][j].lower().replace(\" \",\"\")==rets['table_names'][t].lower().replace(\" \",\"\")):\n",
    "                col_name_temp+=\"Column: \"+data['Column Name'][j].lower().replace(\" \",\"\")+\"; Description: \"+data['Description'][j]+\"\\n\"\n",
    "                code_gen_prompt+=col_name_temp\n",
    "                break\n",
    "\n",
    "    #After adding all table and rule information, tell Gemini how to deal with the two cases of: (1) having enough table information to generate a complete\n",
    "    #SQL query to check for non-compliance with the rule, and (2) having insufficient table information to generate a complete SQL query.\n",
    "    code_gen_prompt+=\"\\nDo one of two things:\\n\" \n",
    "\n",
    "    #If there is sufficient table information, tell Gemini to generate the SQL query along with an explanation for how the given query determines that\n",
    "    #entries are non-compliant with the given rule    \n",
    "    code_gen_prompt+=\"(1) If the columns of \"+tab_name_text+\" are sufficent to generate a complete SQL query that checks for non-compliance\"\n",
    "    code_gen_prompt+=\" of the given rule, generate that SQL query along with an explanation of how it checks for non-compliance.\\n\"\n",
    "    #If there is insufficient table information, tell Gemini to provide a message indicating that a complete SQL query cannot be generated along with an explanation \n",
    "    #of what additional information needs to be provided in order to generate a complete query  \n",
    "    code_gen_prompt+=\"(2) If the columns of \"+tab_name_text+\" are not sufficent to generate a complete SQL query that checks for non-compliance \"\n",
    "    code_gen_prompt+=\"of the given rule, do not generate a SQL query but do generate a message indicating that a complete query cannot be generated\"\n",
    "    code_gen_prompt+=\"  and explain what additional columns are required to generate a complete query.\\n\"\n",
    "\n",
    "    #Tell Gemini that the output should take a JSON form that can be further processed\n",
    "    code_gen_prompt+=\"\\nThe format of the output should take the .JSON form:\\n\"\n",
    "    #Provide an explicit example of what the output should look like so that the key names for all of the JSON entries take a consistent form\n",
    "    code_gen_prompt+='json\"\"\"{\"rule\":'+rets['rule']+',\"type\":\"ExampleType\",\"content\":\"ExampleContent\",\"explanation\":\"ExampleExplanation\"}\"\"\"\\n'\n",
    "    code_gen_prompt+='where the \"type\" entry indicates whether the content entry is a \"message\" or \"SQLquery\", the \"content\" entry contains the SQL '\n",
    "    code_gen_prompt+='query or the message indicating that a complete query cannot be generated and the \"explanation\" entry contains the explanation'\n",
    "    #Tell Gemini to include no further output beyond the JSO\n",
    "    code_gen_prompt+=\" for how the query checks for compliance or what columns are missing. Include no additional output.\"\n",
    "\n",
    "    #Feed the prompt as built above to Gemini\n",
    "    response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=code_gen_prompt)\n",
    "\n",
    "    #Store the LLM response as a string\n",
    "    SQLout=response.text\n",
    "    #Convert the LLM string to an actual JSON dictionary and return that dictionary\n",
    "    json_sql=json.loads(SQLout[7:len(extracted_rules)-3])\n",
    "    return json_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pipeline Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main() function takes no arguments and returns no output. The main() function uses the knowledge graph creation functions, retrieval functions, and\n",
    "#code generation functions provided above to implement a complete pipeline for the knowledge graph financial regulatory compliance project.\n",
    "def main():\n",
    "    #Load the data dictionary into a pandas dataframe\n",
    "    df=pd.read_excel(tab_file)\n",
    "    #Assert whether the graph being used take the semantic or structural form\n",
    "    graph_type=\"structural\" #\"semantic\" or \"structural\"\n",
    "    create_update_graph=False #This boolean is True if the given graph needs to be created or updated. It is False otherwise.\n",
    "    #Assert whether the retriever is the \"table\" type or the \"graph\" type\n",
    "    retriever_type=\"graph\" #\"table\" or \"graph\"\n",
    "\n",
    "    NEO4J_URI = \"bolt://localhost:7687\" #Provide the url for the knowledge graph database. The current implementation is for a database local to your machine.\n",
    "    NEO4J_USERNAME = \"neo4j\" #Provide the username for the knowledge graph database. The current implementation is the default.\n",
    "    #Use an if-else statement to assure that the correct password for the type of graph database is used.\n",
    "    if graph_type==\"structural\":\n",
    "        NEO4J_PASSWORD = \"structural\" #Provide the username for the knowledge graph database. The current implementation is for the structural graph.\n",
    "    elif graph_type==\"semantic\":\n",
    "        NEO4J_PASSWORD = \"semantic\" #Provide the username for the knowledge graph database. The current implementation is for the semantic graph.\n",
    "    kg = Neo4jGraph(url=NEO4J_URI,username=NEO4J_USERNAME,password=NEO4J_PASSWORD) #Access the desired knowledge graph\n",
    "\n",
    "    #Call the knowledge graph creation functions if needed\n",
    "    if create_update_graph:\n",
    "        #Use an if-else statement to determine if the structural or semantic graph is to be created/updated\n",
    "        if graph_type==\"structural\":\n",
    "            dict1=buildJSONStructural(pdf_file) #Generate the JSON file for the knowledge graph\n",
    "            query1=neo4jFromJSONStructural(dict1) #Process the JSON into Cypher queries\n",
    "            kg.query(query1[0]) #Call the node generation Cypher queries\n",
    "            kg.query(query1[1]) #Call the relationship generation Cypher queries\n",
    "        elif graph_type==\"semantic\":\n",
    "            dict1=buildJSONSemantic(pdf_file) #Generate the JSON file for the knowledge graph\n",
    "            query1=neo4jFromJSONSemantic(dict1) #Process the JSON into Cypher queries\n",
    "            kg.query(query1[0]) #Use the node generation Cypher queries\n",
    "            kg.query(query1[1]) #Call the relationship generation Cypher queries\n",
    "\n",
    "    #Initialize a list to store all of the rule texts and table information that will iterated through for the code generation calls to the LLM\n",
    "    ret_tab=[]\n",
    "    #Use an if-else statement to differentiate how the retrieval is handled for either the \"table\" or the \"graph\" retriever\n",
    "    if retriever_type==\"table\":\n",
    "        res=kg.query(\"MATCH (r:Rule) RETURN r\") #Access all of the rule nodes of the knowledge graph\n",
    "        retriever=tableRetrieverInit(tab_file) #Initialize the \"table\" retriever\n",
    "        proc_tab=[] #Initialize a list that will contain the rule text that is fed to each invocation of the retriever\n",
    "        for ru in res:\n",
    "            proc_tab.append(ru['r']['text']) #Append the text of each node to the list\n",
    "        for p in range(len(proc_tab)):\n",
    "            tab1=[] #Initialize a list to hold the table names that are retrieved by the invocation of the \"table retriever\"\n",
    "            tab1=invokeRetriver(retriever,retriever_type,proc_tab[p],5) #Invoke the \"table\" retriever\n",
    "            ret_tab.append({'rule':res[p]['r']['text'],'table_names':tab1}) #Add the rule text and list of table names to the list that will be used in code generation\n",
    "    elif retriever_type==\"graph\":\n",
    "        retriever=graphRetrieverInit(NEO4J_URI,NEO4J_USERNAME,NEO4J_PASSWORD) #Initialize the \"graph\" retriever\n",
    "        tab_ls=[] #Initialize a list that contains all of the unique table names from the data dictionary\n",
    "        for j in df['Table Name']: #Iterate over all of the table name entries of the data dictionary\n",
    "            if j not in tab_ls:\n",
    "                tab_ls.append(j) #If a table name is not in the unique list, add it to the list\n",
    "        \n",
    "        tab_inds=[i for i in range(len(tab_ls))] #Associate a numeric value from 0 to len(tab_ls)-1 with each table name in the list of unique table names\n",
    "        for sub in combinations(tab_inds,1): #Iterate over the combinations of the table names for a given number of tables. Currently set to 1.\n",
    "            tab1=[] #Initialize an empty list that will contain the current combination of table names\n",
    "            for s in sub:\n",
    "                tab1.append(tab_ls[s]) #Add the appropriate names to the list for the current combinations\n",
    "            text=\"\" #Initialize an empty string that will contain all of the text associated with the current combination of tables\n",
    "            for j in range(len(df['Table Name'])):\n",
    "                if df['Table Name'][j] in tab1:\n",
    "                    #Add the column information to the string if it is a element of one of the tables in the current combination\n",
    "                    text+=\"Table: \"+df['Table Name'][j]+\" Column: \"+df['Column Name'][j]+\" Description: \"+df['Description'][j]+\"\\n\"\n",
    "            res=invokeRetriver(retriever,retriever_type,text,5) #Invoke the \"graph\" retriever\n",
    "            #Iterate over the retrieved rules \n",
    "            for ru in res:\n",
    "                ret_tab.append({'rule':ru,'table_names':tab1}) #Add the rule text and list of table names to the list that will be used in code generation\n",
    "    \n",
    "    json_outs=[] #Initialize an empty list that will hold all of the output JSON codes from each unique call of the code generator\n",
    "    for m in range(5):\n",
    "        respon=codeGenerator(df,ret_tab[m]) #Call the code generator for a given rule text and list of table names\n",
    "        r_text=respon.text #Load the response from the LLM into a string\n",
    "        json_outs.append(json.loads(r_text[7:len(r_text)-3])) #Use json.loads to convert the output string to a JSON dictionary and append it to the list\n",
    "        print(json_outs[m]['content']+\"\\n\") #Print the SQL query or inability to generate SQL message for the given call to the code generato\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT transactionid FROM transactionsfacttable WHERE transactionamount > 5000 AND transactionapproval = 0;\n",
      "\n",
      "A complete SQL query cannot be generated to check for non-compliance of the given rule with the available columns in the transactionsfacttable.\n",
      "\n",
      "SELECT cardholderid, DATE(transactionstartdatetime), SUM(transactionamount) AS daily_total\n",
      "FROM transactionsfacttable\n",
      "GROUP BY cardholderid, DATE(transactionstartdatetime)\n",
      "HAVING SUM(transactionamount) > 20000;\n",
      "\n",
      "SELECT transactionid FROM transactionsfacttable WHERE refundscancellations = 1 AND transactionenddatetime IS NOT NULL AND transactionamount != -5.00;\n",
      "\n",
      "A complete query cannot be generated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main() #Call the main() function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
